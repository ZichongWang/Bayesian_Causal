{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import scipy.io as sio\n",
    "from rasterio.enums import Resampling\n",
    "import os\n",
    "from pruning_py import pruning\n",
    "from SVI_py import svi\n",
    "np.random.seed(42)\n",
    "# 初始化\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 导入地理数据\n",
    "location = '/Users/lixintong/Desktop/大三上/大数据并行计算/并行大作业/第三次作业/Bayesian_Causal-main/data'  # 文件所在的位置\n",
    "\n",
    "event = '2023_turkey_new'\n",
    "# event = '2024_japan2'\n",
    "if event == '2023_turkey_new':\n",
    "    event_1 = '2023_turkey'\n",
    "else:\n",
    "    event_1 = event\n",
    "\n",
    "def read_raster(file_path):\n",
    "    \"\"\"\n",
    "    读取栅格数据文件，并返回数据和其元数据配置\n",
    "    参数:\n",
    "        file_path: 栅格文件路径\n",
    "    返回:\n",
    "        data: 栅格数据\n",
    "        profile: 栅格文件的元数据配置\n",
    "    \"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        data = src.read(1, resampling=Resampling.nearest)  # 读取数据，并使用最近邻重采样\n",
    "        profile = src.profile  # 获取文件元数据\n",
    "    return data, profile\n",
    "\n",
    "# 读取多个栅格文件\n",
    "Y, Y_profile = read_raster(os.path.join(location, event, 'damage_proxy_map', f'{event_1}_damage_proxy_map.tif'))\n",
    "BD, BD_profile = read_raster(os.path.join(location, event, 'building_footprint', f'{event_1}_building_footprint_rasterized.tif'))\n",
    "LS, LS_profile = read_raster(os.path.join(location, event, 'prior_models', f'{event_1}_prior_landslide_model.tif'))\n",
    "LF, LF_profile = read_raster(os.path.join(location, event, 'prior_models', f'{event_1}_prior_liquefaction_model.tif'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据修正\n",
    "BD[BD > 0] = 1  # 将基础数据 BD 中所有大于 0 的值设为 1\n",
    "Y[np.isnan(Y)] = 0  # 将 Y 数据中的 NaN 值设为 0\n",
    "BD[np.isnan(BD)] = 0  # 将 BD 数据中的 NaN 值设为 0\n",
    "LS[np.isnan(LS)] = 0  # 将滑坡数据 LS 中的 NaN 值设为 0\n",
    "LF[np.isnan(LF)] = 0  # 将液化数据 LF 中的 NaN 值设为 0\n",
    "Y = (Y + 11) / 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始处理方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/j8zrcsfx0cv0hwpd327473r40000gn/T/ipykernel_66685/312880237.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  new_LS[idx] = np.real(real_roots)  # 将实数根存储到 new_LS，不取最大值\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Landslide Areal Percentages to Probabilities\n",
      "Elapsed time: 445.772633 seconds\n",
      "Converted Liquefaction Areal Percentages to Probabilities\n",
      "Elapsed time: 449.777730 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()  # 开始计时，记录代码执行时间\n",
    "\n",
    "# 将滑坡区域百分比转换为概率\n",
    "new_LS = np.copy(LS)  # 创建滑坡数据的副本\n",
    "index = np.where(LS > 0)  # 找到 LS 中大于 0 的元素索引\n",
    "for i in range(len(index[0])):\n",
    "    idx = (index[0][i], index[1][i])  # 获取当前元素的索引\n",
    "    p = [4.035, -3.042, 5.237, (-7.592 - np.log(LS[idx]))]  # 构建多项式方程\n",
    "    tmp_root = np.roots(p)  # 求解方程的根\n",
    "    real_roots = tmp_root[np.iscomplex(tmp_root) == False]  # 筛选出实数根\n",
    "    new_LS[idx] = np.real(real_roots)  # 将实数根存储到 new_LS，不取最大值\n",
    "print('Converted Landslide Areal Percentages to Probabilities')  # 输出转换完成的信息\n",
    "print(\"Elapsed time: {:.6f} seconds\".format(time.time() - start_time))  # 输出当前已耗时间\n",
    "\n",
    "# 将液化区域百分比转换为概率\n",
    "new_LF = np.copy(LF)  # 创建液化数据的副本\n",
    "index = np.where(LF > 0)  # 找到 LF 中大于 0 的元素索引\n",
    "for i in range(len(index[0])):\n",
    "    idx = (index[0][i], index[1][i])  # 获取当前元素的索引\n",
    "    new_LF[idx] = (np.log((np.sqrt(0.4915 / LF[idx]) - 1) / 42.40)) / (-9.165)  # 根据给定公式计算概率\n",
    "    # new_LF[i] = (np.log((np.sqrt(0.4915 / LF[i]) - 1) / 42.40)) / (-9.165)\n",
    "print('Converted Liquefaction Areal Percentages to Probabilities')  # 输出转换完成的信息\n",
    "print(\"Elapsed time: {:.6f} seconds\".format(time.time() - start_time))  # 输出当前已耗时间\n",
    "\n",
    "# 将概率值转换为非负数\n",
    "new_LF[new_LF < 0] = 0  # 将 new_LF 中小于 0 的值设为 0\n",
    "new_LS[new_LS < 0] = 0  # 将 new_LS 中小于 0 的值设为 0\n",
    "new_LS[np.isnan(new_LS)] = 0  # 将 new_LS 中的 NaN 值设为 0\n",
    "new_LF[np.isnan(new_LF)] = 0  # 将 new_LF 中的 NaN 值设为 0\n",
    "tmp_LF = new_LF  # 临时存储液化数据\n",
    "tmp_LS = new_LS  # 临时存储滑坡数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入的 LS 是一个 4816×5323 的栅格数据，总像元数约 $2.56 \\times 10^7$。其中即使只有一部分像元 >0，仍然会有几百万级别的像元需要求解多项式。\n",
    "\n",
    "- np.roots 逐像元调用\n",
    "  - 即使用了一定的“批量方式”（先批量构造系数，再用列表推导 [np.roots(poly) for poly in p]），本质上仍是对“每一个像元”都单独调用一次三次方程求解。\n",
    "  - 三次方程求解需要做多步复杂的浮点计算，量级大时（几百万次）非常耗时。\n",
    "  - Python 的循环或列表推导在面对上千万次的调用时开销相当大。\n",
    "- 后续逐像元循环赋值\\\n",
    "  - for i, (x, y) in enumerate(zip(*np.where(index))): ...\n",
    "  - 这里又是一个 Python 级的循环，enumerate 一下子就会迭代出上百万乃至几百万次。\n",
    "  - 在循环中做数组索引与赋值操作，频繁地在 Python 和 NumPy 之间转换，也会带来大量开销。\n",
    "- NumPy 机制限制\n",
    "- NumPy 最擅长的是大批量向量化运算（一次性对大数组进行简单算术或逻辑操作）。然而 np.roots 本身并不是真正的“矢量化”函数；它只是可以对 1D 多项式系数数组执行运算。批量执行三次方程求根时，实际还是对每个多项式逐一做同样的运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Landslide Areal Percentages to Probabilities\n",
      "Elapsed time for Landslide: 424.402355 seconds\n",
      "Converted Liquefaction Areal Percentages to Probabilities\n",
      "Elapsed time for Liquefaction: 0.140815 seconds\n",
      "Total elapsed time: 0.195449 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # 开始计时\n",
    "\n",
    "# 滑坡区域百分比转换为概率（向量化实现）\n",
    "def landslide_conversion(LS):\n",
    "    \"\"\"\n",
    "    滑坡概率转换函数，使用向量化处理\n",
    "    \"\"\"\n",
    "    index = LS > 0  # 找到 LS > 0 的位置\n",
    "    log_LS = np.log(LS[index])  # 取对数\n",
    "    n = len(log_LS)  # 有效点的数量\n",
    "\n",
    "    # 构造多项式系数，每行表示一个多项式\n",
    "    p = np.column_stack((\n",
    "        np.full(n, 4.035),       # 多项式最高次项\n",
    "        np.full(n, -3.042),      # 二次项系数\n",
    "        np.full(n, 5.237),       # 一次项系数\n",
    "        -7.592 - log_LS          # 常数项\n",
    "    ))\n",
    "\n",
    "    # 批量求解多项式的根\n",
    "    roots = np.array([np.roots(poly) for poly in p])  # 每一行解一个多项式\n",
    "    real_roots = np.real(roots)  # 提取实数部分的根\n",
    "\n",
    "    # 只取第一个实根\n",
    "    result = np.zeros_like(LS)\n",
    "    for i, (x, y) in enumerate(zip(*np.where(index))):\n",
    "        result[x, y] = real_roots[i][np.isfinite(real_roots[i])].min(initial=0)  # 防止无根情况\n",
    "\n",
    "    return result\n",
    "\n",
    "# 液化区域百分比转换为概率（向量化实现）\n",
    "def liquefaction_conversion(LF):\n",
    "    index = LF > 0\n",
    "    result = np.zeros_like(LF)\n",
    "    result[index] = (np.log((np.sqrt(0.4915 / LF[index]) - 1) / 42.40)) / (-9.165)\n",
    "    result[result < 0] = 0  # 去掉负值\n",
    "    result[~np.isfinite(result)] = 0  # 排除无穷大和 NaN\n",
    "    return result\n",
    "\n",
    "# 滑坡概率转换\n",
    "start_time = time.time()\n",
    "new_LS = landslide_conversion(LS)\n",
    "print(\"Converted Landslide Areal Percentages to Probabilities\")\n",
    "print(f\"Elapsed time for Landslide: {time.time() - start_time:.6f} seconds\")\n",
    "\n",
    "# 液化概率转换\n",
    "start_time = time.time()\n",
    "new_LF = liquefaction_conversion(LF)\n",
    "print(\"Converted Liquefaction Areal Percentages to Probabilities\")\n",
    "print(f\"Elapsed time for Liquefaction: {time.time() - start_time:.6f} seconds\")\n",
    "\n",
    "# 最终清理结果\n",
    "new_LF[new_LF < 0] = 0\n",
    "new_LS[new_LS < 0] = 0\n",
    "new_LS[np.isnan(new_LS)] = 0\n",
    "new_LF[np.isnan(new_LF)] = 0\n",
    "\n",
    "tmp_LF = new_LF\n",
    "tmp_LS = new_LS\n",
    "print(f\"Total elapsed time: {time.time() - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiprocessing 多进程并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-52:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'landslide_conversion_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-50:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'landslide_conversion_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-51:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'landslide_conversion_chunk' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/lixintong/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'landslide_conversion_chunk' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 102\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# 假设 LS, LF 是已加载的 np.array\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# LS, LF = ...\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 102\u001b[0m     new_LS \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_landslide_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverted Landslide Areal Percentages to Probabilities (Parallel)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed time for Landslide: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 68\u001b[0m, in \u001b[0;36mparallel_landslide_conversion\u001b[0;34m(LS, num_workers, chunks)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 2) 建立进程池并行执行\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m---> 68\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlandslide_conversion_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunked_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# 3) 合并各子块结果\u001b[39;00m\n\u001b[1;32m     71\u001b[0m final_result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(LS, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_env/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_env/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_env/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def landslide_conversion_chunk(chunk_data):\n",
    "    \"\"\"\n",
    "    子进程执行的函数:\n",
    "    - 输入 chunk_data: (LS_sub, top_left_x, top_left_y)\n",
    "        其中 LS_sub 是原始数组的一部分\n",
    "    - 返回和 LS_sub 同维度的结果局部矩阵\n",
    "    \"\"\"\n",
    "    LS_sub, offset_x, offset_y = chunk_data\n",
    "    \n",
    "    # --- 这里可以直接复用原始的 landslide_conversion 逻辑 ---\n",
    "    # 但要改成仅对 LS_sub 进行处理\n",
    "    index = LS_sub > 0\n",
    "    log_LS = np.log(LS_sub[index])\n",
    "    n = len(log_LS)\n",
    "\n",
    "    # 构造多项式系数\n",
    "    p = np.column_stack((\n",
    "        np.full(n, 4.035),\n",
    "        np.full(n, -3.042),\n",
    "        np.full(n, 5.237),\n",
    "        -7.592 - log_LS\n",
    "    ))\n",
    "\n",
    "    # 批量求根\n",
    "    roots = np.array([np.roots(poly) for poly in p])\n",
    "    real_roots = np.real(roots)\n",
    "\n",
    "    # 写回结果\n",
    "    sub_result = np.zeros_like(LS_sub, dtype=float)\n",
    "    coords = np.argwhere(index)  # 在子块坐标系下\n",
    "    for i, (x, y) in enumerate(coords):\n",
    "        valid_vals = real_roots[i][np.isfinite(real_roots[i])]\n",
    "        if valid_vals.size > 0:\n",
    "            sub_result[x, y] = valid_vals.min()\n",
    "        else:\n",
    "            sub_result[x, y] = 0.0\n",
    "\n",
    "    return (sub_result, offset_x, offset_y)\n",
    "\n",
    "\n",
    "def parallel_landslide_conversion(LS, num_workers=None, chunks=4):\n",
    "    \"\"\"\n",
    "    使用多进程并行对 LS 做 landslide_conversion\n",
    "    - num_workers: 并行进程数(默认是 cpu_count())\n",
    "    - chunks: 在维度上分块的数量(示例简化为对行方向分块)\n",
    "    \"\"\"\n",
    "    if num_workers is None:\n",
    "        num_workers = cpu_count()\n",
    "    \n",
    "    # 1) 将 LS 在行方向做拆分\n",
    "    rows = LS.shape[0]\n",
    "    chunk_size = rows // chunks\n",
    "    chunked_data = []\n",
    "    for i in range(chunks):\n",
    "        start = i * chunk_size\n",
    "        # 最后一块可能包含多余行\n",
    "        end = rows if (i == chunks - 1) else ((i+1) * chunk_size)\n",
    "        LS_sub = LS[start:end, :]\n",
    "        # 把子块和它在原图中的起始偏移量一起传递\n",
    "        chunked_data.append((LS_sub, start, 0))\n",
    "\n",
    "    # 2) 建立进程池并行执行\n",
    "    with Pool(processes=num_workers) as p:\n",
    "        results = p.map(landslide_conversion_chunk, chunked_data)\n",
    "\n",
    "    # 3) 合并各子块结果\n",
    "    final_result = np.zeros_like(LS, dtype=float)\n",
    "    for sub_result, offset_x, offset_y in results:\n",
    "        h, w = sub_result.shape\n",
    "        final_result[offset_x:offset_x+h, offset_y:offset_y+w] = sub_result\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def liquefaction_conversion(LF):\n",
    "    \"\"\"\n",
    "    液化概率转换函数（保持不变）\n",
    "    \"\"\"\n",
    "    index = LF > 0\n",
    "    result = np.zeros_like(LF, dtype=float)\n",
    "    result[index] = (\n",
    "        np.log( (np.sqrt(0.4915 / LF[index]) - 1) / 42.40 )\n",
    "        / (-9.165)\n",
    "    )\n",
    "    # 去掉负值、无穷大和 NaN\n",
    "    result[result < 0] = 0\n",
    "    result[~np.isfinite(result)] = 0\n",
    "    return result\n",
    "\n",
    "# ------------------------------#\n",
    "#      主程序: 使用并行加速\n",
    "# ------------------------------#\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设 LS, LF 是已加载的 np.array\n",
    "    # LS, LF = ...\n",
    "\n",
    "    start_time = time.time()\n",
    "    new_LS = parallel_landslide_conversion(LS, num_workers=4, chunks=4)\n",
    "    print(\"Converted Landslide Areal Percentages to Probabilities (Parallel)\")\n",
    "    print(f\"Elapsed time for Landslide: {time.time() - start_time:.6f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    new_LF = liquefaction_conversion(LF)\n",
    "    print(\"Converted Liquefaction Areal Percentages to Probabilities\")\n",
    "    print(f\"Elapsed time for Liquefaction: {time.time() - start_time:.6f} seconds\")\n",
    "\n",
    "    # 清理结果\n",
    "    new_LF[new_LF < 0] = 0\n",
    "    new_LS[new_LS < 0] = 0\n",
    "    new_LS[np.isnan(new_LS)] = 0\n",
    "    new_LF[np.isnan(new_LF)] = 0\n",
    "\n",
    "    print(f\"Total elapsed time: {time.time() - start_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "three_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
